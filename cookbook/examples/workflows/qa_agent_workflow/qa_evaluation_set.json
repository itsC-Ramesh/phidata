[
    {
        "Question": "What is the role of planning in LLM-powered autonomous agents?",
        "Answer": "Planning involves breaking down large tasks into manageable subgoals and refining actions through self-reflection, enabling efficient task handling and iterative improvement."
    },
    {
        "Question": "How does Chain of Thought (CoT) prompting enhance model performance?",
        "Answer": "CoT instructs models to think step by step, decomposing complex tasks into simpler steps, which improves reasoning and problem-solving capabilities."
    },
    {
        "Question": "What is the difference between zero-shot and few-shot learning in prompt engineering?",
        "Answer": "Zero-shot learning provides the model with a task without prior examples, while few-shot learning includes a few examples to guide the model, often leading to better performance."
    },
    {
        "Question": "What are adversarial attacks on large language models (LLMs)?",
        "Answer": "Adversarial attacks involve inputs designed to trigger LLMs to produce undesired outputs, potentially compromising safety and alignment."
    },
    {
        "Question": "How can token manipulation be used as an adversarial attack method?",
        "Answer": "By altering a small fraction of tokens in the input text, attackers can cause the model to fail while maintaining the original semantic meaning."
    },
    {
        "Question": "What is the purpose of self-reflection in autonomous agents?",
        "Answer": "Self-reflection allows agents to critique and learn from past actions, refining future decisions to improve task outcomes."
    },
    {
        "Question": "What is the significance of memory in LLM-powered autonomous agents?",
        "Answer": "Memory enables agents to retain and recall information over time, supporting learning, adaptation, and informed decision-making."
    },
    {
        "Question": "How does Tree of Thoughts (ToT) extend the concept of Chain of Thought (CoT)?",
        "Answer": "ToT explores multiple reasoning possibilities at each step, creating a tree structure that allows for broader exploration of potential solutions."
    },
    {
        "Question": "What is the purpose of self-consistency sampling in prompt engineering?",
        "Answer": "Self-consistency sampling involves generating multiple outputs and selecting the most consistent one, enhancing the reliability of the model's responses."
    },
    {
        "Question": "How do gradient-based attacks differ from token manipulation attacks on LLMs?",
        "Answer": "Gradient-based attacks rely on gradient signals to craft adversarial inputs, typically requiring white-box access, whereas token manipulation alters input tokens without gradient information, often in black-box settings."
    }
]